
\chapter{Chapter 2: Abstraction \newline \textit{Regression \& Tree Models}}


```{r eval=TRUE}
# Simple example of regression with one predictor
data = data.frame(rbind(c(1,2),c(3,3),c(3,5),
                        c(5,4),c(5,6),c(6,5),
                        c(8,7),c(9,8)))
colnames(data) = c("Y","X")
str(data)
lm.YX <- lm(Y ~ X, data = data)
summary(lm.YX)
```


```{r eval=TRUE, tidy = FALSE}
# Step 1 -> Read data into R workstation
# RCurl is the R package to read csv file using a link
library(RCurl)
url <- paste0("https://raw.githubusercontent.com",
              "/analyticsbook/book/main/data/AD.csv")
AD <- read.csv(text=getURL(url))
# str(AD)
```


```{r eval=TRUE, tidy = FALSE}
# Step 2 -> Data preprocessing.
# Remove variable DX_bl
AD <- AD[ , -which(names(AD) %in% c("DX_bl"))] 
# Pick up the first 15 variables for predictors
X <- AD[,1:15]
# Pick up the variable MMSCORE for outcome
Y <- AD$MMSCORE
```


```{r eval=TRUE}
data <- data.frame(X,Y)
names(data)[16] <- c("MMSCORE")
```


```{r eval=TRUE}
set.seed(1) # generate the same random sequence
# Create a training data (half the original data size)
train.ix <- sample(nrow(data),floor( nrow(data)/2) )
data.train <- data[train.ix,]
# Create a testing data (half the original data size)
data.test <- data[-train.ix,]
```


```{r eval=TRUE}
# Step 3 -> Use lm() function to build a full 
# model with all predictors
lm.AD <- lm(MMSCORE ~ ., data = data.train)
summary(lm.AD)
```


```{r eval=TRUE}
# Step 4 -> use step() to automatically delete 
# all the insignificant variables
# Automatic model selection
lm.AD.reduced <- step(lm.AD, direction="backward", test="F")
```

```{r eval=TRUE}
anova(lm.AD.reduced,lm.AD)
```


```{r eval=TRUE}
# Step 5 -> Predict using your linear regession model
pred.lm <- predict(lm.AD.reduced, data.test)
# For regression model, you can use correlation to measure 
# how close your predictions with the true outcome 
# values of the data points
cor(pred.lm, data.test$MMSCORE)
```


```{r eval=TRUE, tidy = FALSE, size = 'small'}
# Scatterplot matrix to visualize the relationship
# between outcome variable with continuous predictors
library(ggplot2)
# install.packages("GGally")
library(GGally)
# draw the scatterplots and also empirical
# shapes of the distributions of the variables
p <- ggpairs(AD[,c(16,1,3,4,5,6)],
             upper = list(continuous = "points"),
             lower = list(continuous = "cor"))
print(p)

library(ggplot2)
qplot(factor(PTGENDER),
      MMSCORE, data = AD,geom=c("boxplot"), fill = factor(PTGENDER))
```


```{r eval=TRUE}
# How to detect interaction terms
# by exploratory data analysis (EDA)
require(ggplot2)
p <- ggplot(AD, aes(x = PTEDUCAT, y = MMSCORE))
p <- p + geom_point(aes(colour=AGE), size=2)
# p <- p + geom_smooth(method = "auto")
p <- p + labs(title="MMSE versus PTEDUCAT")
print(p)
```



```{r eval=TRUE}
p <- ggplot(AD[which(AD$AGE < 60),], 
            aes(x = PTEDUCAT, y = MMSCORE))
p <- p + geom_point(size=2)
p <- p + geom_smooth(method = lm)
p <- p + labs(title="MMSE versus PTEDUCAT when AGE < 60")
print(p)
```

```{r eval=TRUE}
p <- ggplot(AD[which(AD$AGE > 80),], 
            aes(x = PTEDUCAT, y = MMSCORE))
p <- p + geom_point(size=2)
p <- p + geom_smooth(method = lm)
p <- p + labs(title="MMSE versus PTEDUCAT when AGE > 80")
print(p)
```


```{r eval=TRUE}
# fit the multiple linear regression model 
# with an interaction term: AGE*PTEDUCAT
lm.AD.int <- lm(MMSCORE ~ AGE + PTGENDER + PTEDUCAT 
                  + AGE*PTEDUCAT, data = AD)
summary(lm.AD.int)
```


```{r eval=TRUE, tidy=FALSE}
# Key package for decision tree in R: 
# rpart (for building the tree); 
# rpart.plot (for drawing the tree)
library(RCurl)
library(rpart)
library(rpart.plot)

# Step 1 -> Read data into R workstation
url <- paste0("https://raw.githubusercontent.com",
              "/analyticsbook/book/main/data/AD.csv")
data <- read.csv(text=getURL(url))
```

\textbf{Step 2} is about data preprocessing.

```{r eval=TRUE, tidy=FALSE}
# Step 2 -> Data preprocessing
# Create your X matrix (predictors) and 
# Y vector (outcome variable)
X <- data[,2:16]
Y <- data$DX_bl

# The following code makes sure the variable "DX_bl" 
# is a "factor".
Y <- paste0("c", Y) 
# This line is to "factorize" the variable "DX_bl".
# It denotes "0" as "c0" and "1" as "c1",
# to highlight the fact that
# "DX_bl" is a factor variable, not a numerical variable
Y <- as.factor(Y) # as.factor is to convert any variable
                  # into the format as "factor" variable.

# Then, we integrate everything into a data frame
data <- data.frame(X,Y)
names(data)[16] = c("DX_bl")

set.seed(1) # generate the same random sequence
# Create a training data (half the original data size)
train.ix <- sample(nrow(data),floor( nrow(data)/2) )
data.train <- data[train.ix,]
# Create a testing data (half the original data size)
data.test <- data[-train.ix,]
```


\textbf{Step 3} is to use the `rpart()` function in the R package `rpart` to build the decision tree.
```{r eval=TRUE, tidy=FALSE}
# Step 3 -> use rpart to build the decision tree.
tree <- rpart(DX_bl ~ ., data = data.train)
```


```{r eval=TRUE}
# Step 4 -> draw the tree
prp(tree, nn.cex = 1)
```


```{r eval=TRUE}
# Step 5 -> prune the tree
tree <- prune(tree,cp=0.03)
prp(tree,nn.cex=1)
```


```{r eval=TRUE, tidy=FALSE}
# Step 6 -> Predict using your tree model
pred.tree <- predict(tree, data.test, type="class")
```


```{r eval=TRUE,tidy = FALSE, size = 'small'}
# The following line calculates the prediction error
# rate (a number from 0 to 1) for a binary classification problem
err.tree <- length(which(pred.tree !=
                           data.test$DX_bl))/length(pred.tree)
# 1) which(pred.tree != data$DX_bl) identifies the locations
#    of the incorrect predictions;
# 2) length(any vector) returns the length of that vector;
# 3) thus, the ratio of incorrect prediction over the total
#    prediction is the prediction error
print(err.tree)
```


```{r eval=TRUE}
lm.AD.age <- lm(MMSCORE ~ AGE, data = AD)
summary(lm.AD.age)
```

```{r eval=TRUE, tidy = FALSE}
# fit the multiple linear regression model 
# with more than one predictor
lm.AD.demo <- lm(MMSCORE ~  AGE + PTGENDER + PTEDUCAT,
                  data = AD)
summary(lm.AD.demo)
```



